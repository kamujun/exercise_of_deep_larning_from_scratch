{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6章 学習に関するテクニック\n",
    "\n",
    "## 6.1 パラメータの更新\n",
    "ニューラルネットワークの学習の目的は損失関数の値をできるだけ小さくするパラメータを見つけることである。このような問題を解くことを「最適化(optimization)」という。\n",
    "これまで確率的勾配降下法(SGD)で解いてきたが、問題によってはSGDより優れる最適化手法が存在する。\n",
    "\n",
    "### 6.1.1 冒険家の話\n",
    "割愛\n",
    "\n",
    "### 6.1.2 SGD\n",
    "SGDの復習。SGDは以下式で表せる。\n",
    "$W ← W-\\eta\\frac{\\partial L}{\\partial W}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 SGDの欠点\n",
    "\n",
    "SGDは問題によっては非効率な場合がある。以下関数の最小値を求める問題を考える。\n",
    "\n",
    "$$f(x,y) = \\frac{1}{20}x^{2} + y^{2}$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.05\na\n4.0\na\n4.05\na\n1.05\na\n[  249 15000]\n"
     ]
    }
   ],
   "source": [
    "# cf.http://d.hatena.ne.jp/white_wheels/20100327/p3\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "def _numerical_gradient_no_batch(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 値を元に戻す\n",
    "        \n",
    "    return grad\n",
    "\n",
    "\n",
    "def numerical_gradient(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_no_batch(f, X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        \n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
    "        \n",
    "        return grad\n",
    "\n",
    "\n",
    "def function_2(x):\n",
    "    print(x[0]**2 / 20 + x[1]**2)\n",
    "    return x[0]**2 / 20 + x[1]**2\n",
    "\n",
    "\n",
    "def exp_func(arr):\n",
    "    # FIX ME\n",
    "    ans = arr[0]**2 / 20 + arr[1]**2\n",
    "\n",
    "    return ans\n",
    "  \n",
    "     \n",
    "if __name__ == '__main__':\n",
    "    x = np.arange(-10, 10, 1)\n",
    "    y = np.arange(-10, 10, 1)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    X = X.flatten()\n",
    "    Y = Y.flatten()\n",
    "    \n",
    "    grad = numerical_gradient(function_2, np.array([1, 2]) )\n",
    "    print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}